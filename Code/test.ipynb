{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Plot the neural architecture\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Plot the neural architecture\n",
    "\"\"\"\n",
    "\n",
    "# FAB = FABBlock(kernel_size=3, padding=1, out_channels=5)\n",
    "# model = torch.nn.Sequential(FAB)\n",
    "# X_test = torch.randn(2, 3, 10, 10)\n",
    "# torch.onnx.export(model, X_test, 'FAB.onnx', input_names=[\"input_features\"], output_names=[\"output_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 48, 15, 15])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 256, 256)\n",
    "decoder = EnCNNBlock(3, h_num=4)\n",
    "decoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 48, 15, 15)\n",
    "encoder = DeCNNBlock(48, h_num=4)\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 48, 15, 15)\n",
    "FAB = FABBlock(kernel_size=3, padding=1, in_channels=48)\n",
    "FAR_2 = FABBlock(kernel_size=3, padding=1, in_channels=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 256, 256)\n",
    "x_grad = torch.randn(2, 3, 256, 256)\n",
    "lie = LIE(3, fab_num=4)\n",
    "lie([x, x_grad]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIE(\n",
       "  (encoder_1): EnCNNBlock(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(12, 24, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (encoder_2): EnCNNBlock(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(12, 24, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (FAB_b): FABBlock(\n",
       "    (conv): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow1): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow2): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (CA): CABlock(\n",
       "      (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (PA): PABlock(\n",
       "      (conv_1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (FAB_b_n): FABBlock(\n",
       "    (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (CA): CABlock(\n",
       "      (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (PA): PABlock(\n",
       "      (conv_1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (FAB_n): FABBlock(\n",
       "    (conv): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow1): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (CA): CABlock(\n",
       "      (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (PA): PABlock(\n",
       "      (conv_1): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): DeCNNBlock(\n",
       "    (decoder): Sequential(\n",
       "      (0): ConvTranspose2d(24, 12, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(12, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose2d(6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 64, 256, 256)\n",
    "torch.nn.functional.adaptive_avg_pool2d(x, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x, dim=[2, 3], keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::uniform' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1236\\3454372050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLIE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfab_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_grad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LIE.onnx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \"\"\"\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m     _export(\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[0;32m   1527\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[0;32m   1530\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m         graph = _optimize_graph(\n\u001b[0m\u001b[0;32m   1116\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[0moperator_export_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[1;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[0;32m    661\u001b[0m     \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_pass_onnx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m     \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\onnx\\utils.py\u001b[0m in \u001b[0;36m_run_symbolic_function\u001b[1;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputsSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1909\u001b[1;33m         raise errors.UnsupportedOperatorError(\n\u001b[0m\u001b[0;32m   1910\u001b[0m             \u001b[0mdomain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1911\u001b[0m             \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::uniform' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256)\n",
    "x_grad = torch.randn(1, 3, 256, 256)\n",
    "lie = LIE(3, fab_num=2)\n",
    "model = torch.nn.Sequential(lie)\n",
    "torch.onnx.export(model, ([x, x_grad]), 'LIE.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIE(\n",
       "  (encoder_1): EnCNNBlock(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(12, 24, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (encoder_2): EnCNNBlock(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(12, 24, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (FAB_b): FABBlock(\n",
       "    (conv): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow1): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow2): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (CA): CABlock(\n",
       "      (GAP): AdaptiveAvgPool3d(output_size=(16, 1, 1))\n",
       "      (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (PA): PABlock(\n",
       "      (conv_1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (FAB_b_n): FABBlock(\n",
       "    (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (CA): CABlock(\n",
       "      (GAP): AdaptiveAvgPool3d(output_size=(16, 1, 1))\n",
       "      (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (PA): PABlock(\n",
       "      (conv_1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (FAB_n): FABBlock(\n",
       "    (conv): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow1): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_dow2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (CA): CABlock(\n",
       "      (GAP): AdaptiveAvgPool3d(output_size=(24, 1, 1))\n",
       "      (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (PA): PABlock(\n",
       "      (conv_1): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): DeCNNBlock(\n",
       "    (decoder): Sequential(\n",
       "      (0): ConvTranspose2d(24, 12, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(12, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose2d(6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
